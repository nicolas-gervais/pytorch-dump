import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.datasets import make_regression
from torch import nn as nn
import torch.nn.functional as F

x, y = make_regression(n_samples=2500, n_features=10, n_targets=5, noise=0)


class TorchDataset(Dataset):
    def __init__(self, inputs, labels):
        self.inputs = inputs
        self.labels = labels

    def __len__(self):
        return len(self.inputs)

    def __getitem__(self, index):
        return self.inputs[index], self.labels[index]


train_dataset = TorchDataset(x[:2000], y[:2000])
test_dataset = TorchDataset(x[2000:], y[2000:])

train_loader = DataLoader(train_dataset, batch_size=8, drop_last=True) # make sampler
test_loader = DataLoader(test_dataset, batch_size=8, drop_last=True)


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.lin1 = nn.Linear(x.shape[1], 16)
        self.lin2 = nn.Linear(self.lin1.out_features, 32)
        self.lin3 = nn.Linear(self.lin2.out_features, 64)
        self.drop1 = nn.Dropout(0.5)
        self.lin4 = nn.Linear(self.lin3.out_features, y.shape[1])

    def forward(self, x):
        x = self.lin1(x)
        x = F.relu(x)
        x = self.lin2(x)
        x = F.relu(x)
        x = self.lin3(x)
        x = F.relu(x)
        x = self.drop1(x)
        x = self.lin4(x)
        return x

net = Net()

example_input, example_labels = iter(train_loader).next()

print(net(example_input.type(torch.FloatTensor)))

device = 'cuda:0' if torch.cuda.is_available() else 'cpu'

loss_fn = nn.L1Loss()

def train(model, device, train_loader, optimizer, loss_fn):
    model.train()
    running_loss = 0.

    for ix, (inputs, labels) in enumerate(train_loader, 1):
        inputs = inputs.type(torch.FloatTensor).to(device)
        target = labels.type(torch.LongTensor).to(device)

        optimizer.zero_grad()
        output = model(inputs)

        loss = loss_fn(target, output)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    return torch.div(running_loss, torch.mul(ix, train_loader.batch_size))


def test(model, device, test_loader, loss_fn):
    model.eval()
    test_loss = 0.
    correct = 0.
    with torch.no_grad():
        for ix, (inputs, labels) in enumerate(test_loader, 1):
            inputs = inputs.type(torch.FloatTensor).to(device)
            target = labels.type(torch.LongTensor).to(device)

            output = model(inputs)
            loss = loss_fn(target, output)

            test_loss += loss.item()
    return torch.div(test_loss, torch.mul(ix, test_loader.batch_size))


net.to(device)

optimizer = torch.optim.Adam(net.parameters(), lr=0.01)

for epoch in range(1, 20 + 1):
    train_loss = train(net, device, train_loader, optimizer, loss_fn)
    test_loss = test(net, device, train_loader, loss_fn)

    print(f'Epoch {epoch:2d} Train Loss {train_loss:=5.3f} Test Loss {test_loss:5.3f}')
