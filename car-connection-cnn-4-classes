import torch
from torchvision import transforms
import torch.nn as nn
import torch.nn.functional as F
from collections import Counter
from sklearn.model_selection import train_test_split
from itertools import groupby
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import LabelEncoder
from numpy.random import choice
from PIL import Image
import os
os.chdir('d:/tcc')

makes = Counter()

for file in os.listdir():
    makes.update([file.split('_')[0]])

most_common = list(map(lambda x: x[0], makes.most_common(4)))

to_keep = list(filter(lambda x: x.split('_')[0] in most_common, os.listdir()))

grouped = [list(group) for key, group in groupby(sorted(to_keep), key=lambda x: x.split('_')[0])]

X = [i for s in map(lambda x: choice(x, 4_000), grouped) for i in s]

labels = list(map(lambda x: x.split('_')[0], X))

y = LabelEncoder().fit_transform(labels)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.1, shuffle=True)


class NeuralNet(nn.Module):
    def __init__(self):
        super(NeuralNet, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=3,
                               out_channels=16,
                               kernel_size=(3, 3))
        self.maxp1 = nn.MaxPool2d(kernel_size=(2, 2))
        self.conv2 = nn.Conv2d(in_channels=self.conv1.out_channels,
                               out_channels=32,
                               kernel_size=(3, 3))
        self.maxp2 = nn.MaxPool2d(kernel_size=(2, 2))
        self.lin1 = nn.Linear(in_features=32*14*14, out_features=128)
        self.lin2 = nn.Linear(in_features=self.lin1.out_features,
                              out_features=len(most_common))

    def forward(self, x):
        x = self.maxp1(F.relu(self.conv1(x)))
        x = self.maxp2(F.relu(self.conv2(x)))
        x = x.view(x.size(0), -1)
        x = F.relu(self.lin1(x))
        x = self.lin2(x)
        return x


net = NeuralNet()
print(net)

class CustomDataSet(Dataset):
    def __init__(self, inputs, outputs, transform):
        self.transform = transform
        self.inputs = inputs
        self.outputs = outputs

    def __len__(self):
        return len(self.inputs)

    def __getitem__(self, idx):
        image = Image.open(self.inputs[idx])
        tensor_image = self.transform(image)
        return tensor_image, self.outputs[idx]

transformations = transforms.Compose([
    transforms.Resize((64,64)),
    transforms.ToTensor()
])

train_data = CustomDataSet(X_train, y_train, transformations)
test_data = CustomDataSet(X_test, y_test, transformations)

train_loader = DataLoader(train_data, shuffle=True, batch_size=32, drop_last=True)
test_loader = DataLoader(test_data, shuffle=True, batch_size=32, drop_last=True)

example_input, example_output = iter(train_loader).next()

print(example_output)
print(example_output.shape)
print(example_input.shape)

with torch.no_grad():
    print(net(example_input).shape)

criterion = nn.CrossEntropyLoss()

optimizer = torch.optim.Adam(net.parameters(), lr=0.001)


if torch.cuda.is_available():
    device = 'cuda:0'
    gpu_name = torch.cuda.get_device_name('cuda')
    processor_count = torch.cuda.get_device_properties('cuda').multi_processor_count
    gpu_total_memory = torch.cuda.get_device_properties('cuda').total_memory
    print('{} is available with {} processors and {:.2f} GB of total memory.'.format(
        gpu_name, processor_count, gpu_total_memory / 1_000_000_000))
else:
    device = 'cpu'

def train():
    net.train()
    running_loss = 0.

    for ix, (inputs, labels) in enumerate(train_loader, 1):
        inputs = inputs.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()
        output = net(inputs)

        loss = criterion(output, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    return torch.div(running_loss, torch.mul(ix, train_loader.batch_size))


def test():
    net.eval()
    test_loss = 0.
    correct = 0.

    with torch.no_grad():
        for ix, (inputs, labels) in enumerate(test_loader, 1):
            inputs = inputs.to(device)
            labels = labels.to(device)
            output = net(inputs)

            loss = criterion(output, labels)
            test_loss += loss.item()

            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(labels.view_as(pred)).sum().item()
    batch_size =test_loader.batch_size
    return test_loss/torch.mul(ix, batch_size), correct/(ix*batch_size)


def main(epochs=1):
    net.to(device)

    for epoch in range(1, epochs + 1):
        train_loss = train()
        test_loss, test_acc = test()

        print(f'Epoch {epoch:2d} Train Loss {train_loss:=6.4f} '
              f'Test Loss {test_loss:6.4f} Test Acc {test_acc:=6.2%}')


if __name__ == '__main__':
    main(epochs=10)


----------
NeuralNet(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))
  (maxp1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))
  (maxp2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
  (lin1): Linear(in_features=6272, out_features=128, bias=True)
  (lin2): Linear(in_features=128, out_features=4, bias=True)
)

tensor([2, 0, 3, 0, 0, 1, 0, 3, 2, 1, 3, 0, 0, 2, 1, 0, 1, 1, 0, 0, 3, 1, 2, 2,
        1, 1, 0, 1, 1, 0, 1, 3])
        
torch.Size([32])

torch.Size([32, 3, 64, 64])

torch.Size([32, 4])

GeForce GTX 960M is available with 5 processors and 2.15 GB of total memory.

Epoch  1 Train Loss 0.0398 Test Loss 0.0000 Test Acc 53.44%
Epoch  2 Train Loss 0.0280 Test Loss 0.0000 Test Acc 67.69%
Epoch  3 Train Loss 0.0166 Test Loss 0.0000 Test Acc 79.00%
Epoch  4 Train Loss 0.0091 Test Loss 0.0000 Test Acc 85.50%
Epoch  5 Train Loss 0.0046 Test Loss 0.0000 Test Acc 85.94%
Epoch  6 Train Loss 0.0025 Test Loss 0.0000 Test Acc 86.31%
Epoch  7 Train Loss 0.0012 Test Loss 0.0000 Test Acc 87.19%
-----------------------------------------------------------
